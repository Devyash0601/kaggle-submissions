{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f60a27a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-19T08:18:23.728287Z",
     "iopub.status.busy": "2025-02-19T08:18:23.727886Z",
     "iopub.status.idle": "2025-02-19T08:18:24.945335Z",
     "shell.execute_reply": "2025-02-19T08:18:24.944002Z"
    },
    "papermill": {
     "duration": 1.223819,
     "end_time": "2025-02-19T08:18:24.947403",
     "exception": false,
     "start_time": "2025-02-19T08:18:23.723584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/digit-recognizer/sample_submission.csv\n",
      "/kaggle/input/digit-recognizer/train.csv\n",
      "/kaggle/input/digit-recognizer/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a6763",
   "metadata": {
    "papermill": {
     "duration": 0.002557,
     "end_time": "2025-02-19T08:18:24.953182",
     "exception": false,
     "start_time": "2025-02-19T08:18:24.950625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1ï¸âƒ£ Introduction\n",
    "Handwritten digit recognition is a classic problem in computer vision and machine learning. It involves classifying grayscale images of handwritten digits (0â€“9) into their respective categories. The MNIST dataset, which consists of 28x28 pixel images of digits, is widely used as a benchmark for this task.\n",
    "\n",
    "In this project, we use Deep Learning with Convolutional Neural Networks (CNNs) to classify these images. CNNs are powerful models that excel at capturing spatial hierarchies in image data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b54531",
   "metadata": {
    "papermill": {
     "duration": 0.002417,
     "end_time": "2025-02-19T08:18:24.958316",
     "exception": false,
     "start_time": "2025-02-19T08:18:24.955899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2ï¸âƒ£ Understanding the Dataset\n",
    "The dataset consists of:\n",
    "\n",
    "Training Data: 42,000 images labeled from 0 to 9 (each image is 28Ã—28 pixels).\n",
    "Test Data: 28,000 images without labels (to be predicted).\n",
    "Each image is represented as a grayscale matrix of shape (28,28,1) where each pixel has an intensity between 0 (black) and 255 (white).\n",
    "\n",
    "## 3ï¸âƒ£ Convolutional Neural Networks (CNNs) Overview\n",
    "A CNN is a deep learning model specifically designed for image recognition. It consists of the following layers:\n",
    "\n",
    "ğŸ”¹ Convolutional Layer\n",
    "Extracts patterns (edges, curves, textures) from the image using filters/kernels.\n",
    "Captures spatial relationships by applying a sliding window operation over the image.\n",
    "ğŸ”¹ Activation Function (ReLU - Rectified Linear Unit)\n",
    "Introduces non-linearity to the model.\n",
    "Converts negative values to zero to maintain positive activations.\n",
    "ğŸ”¹ Pooling Layer (MaxPooling)\n",
    "Reduces dimensionality while retaining important features.\n",
    "Helps in making the model translation-invariant.\n",
    "ğŸ”¹ Fully Connected (Dense) Layers\n",
    "Flattens the output and connects it to Dense layers.\n",
    "The final layer contains 10 neurons (one for each digit 0-9) with a softmax activation function.\n",
    "ğŸ”¹ Dropout Layer\n",
    "Prevents overfitting by randomly turning off some neurons during training.\n",
    "## 4ï¸âƒ£ Data Preprocessing & Augmentation\n",
    "Normalization: Pixel values are scaled between 0 and 1 to speed up training.\n",
    "Reshaping: Images are reshaped from (28,28) to (28,28,1) to fit CNN input requirements.\n",
    "Data Augmentation:\n",
    "Rotation (Â±10Â°): Helps the model generalize across different orientations.\n",
    "Zooming (Â±10%): Simulates variations in digit sizes.\n",
    "Shifting (Â±10%): Accounts for slight positional variations.\n",
    "## 5ï¸âƒ£ Model Architecture Used\n",
    "Our CNN model follows this architecture:\n",
    "\n",
    "Layer\tType\tFilters/Units\tActivation\tPurpose\n",
    "Conv2D\tConvolution\t32 filters (3x3)\tReLU\tExtracts features (edges, patterns)\n",
    "MaxPooling2D\tPooling\t(2x2)\t-\tReduces spatial dimensions\n",
    "Conv2D\tConvolution\t64 filters (3x3)\tReLU\tLearns deeper features\n",
    "MaxPooling2D\tPooling\t(2x2)\t-\tFurther reduces dimensions\n",
    "Flatten\tFlatten\t-\t-\tConverts 2D features into a 1D vector\n",
    "Dense\tFully Connected\t128\tReLU\tLearns complex representations\n",
    "Dropout\tRegularization\t30% neurons off\t-\tReduces overfitting\n",
    "Dense\tFully Connected\t64\tReLU\tMore abstract feature learning\n",
    "Dense\tOutput\t10 neurons\tSoftmax\tPredicts probability for each digit\n",
    "## 6ï¸âƒ£ Training the Model\n",
    "Loss Function: Sparse Categorical Crossentropy\n",
    "Optimizer: Adam (Adaptive Moment Estimation)\n",
    "Metric: Accuracy\n",
    "Batch Size: 64\n",
    "Epochs: 20\n",
    "Data Generator: ImageDataGenerator for augmentation\n",
    "## 7ï¸âƒ£ Model Evaluation\n",
    "We use a train-validation split (90%-10%).\n",
    "Monitor accuracy and loss curves.\n",
    "If validation accuracy is low, tweak hyperparameters.\n",
    "## 8ï¸âƒ£ Making Predictions\n",
    "The model predicts on test images.\n",
    "Converts one-hot encoded outputs to class labels.\n",
    "Generates a submission.csv file with predictions.\n",
    "## 9ï¸âƒ£ Key Takeaways\n",
    "âœ… CNNs excel in image classification tasks.\n",
    "âœ… Data Augmentation improves generalization.\n",
    "âœ… Dropout & Regularization reduce overfitting.\n",
    "âœ… Hyperparameter tuning can further boost accuracy.\n",
    "\n",
    "ğŸ”¹ Next Steps to Improve\n",
    "âœ”ï¸ Use Deeper CNNs (ResNet, EfficientNet).\n",
    "âœ”ï¸ Implement Learning Rate Schedulers.\n",
    "âœ”ï¸ Experiment with Ensemble Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597fab8e",
   "metadata": {
    "papermill": {
     "duration": 0.002429,
     "end_time": "2025-02-19T08:18:24.963429",
     "exception": false,
     "start_time": "2025-02-19T08:18:24.961000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6810a315",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T08:18:24.970258Z",
     "iopub.status.busy": "2025-02-19T08:18:24.969705Z",
     "iopub.status.idle": "2025-02-19T08:28:23.496029Z",
     "shell.execute_reply": "2025-02-19T08:28:23.494839Z"
    },
    "papermill": {
     "duration": 598.531858,
     "end_time": "2025-02-19T08:28:23.497876",
     "exception": false,
     "start_time": "2025-02-19T08:18:24.966018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 41ms/step - accuracy: 0.6774 - loss: 0.9520 - val_accuracy: 0.9586 - val_loss: 0.1283\n",
      "Epoch 2/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 41ms/step - accuracy: 0.9325 - loss: 0.2221 - val_accuracy: 0.9821 - val_loss: 0.0631\n",
      "Epoch 3/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 40ms/step - accuracy: 0.9520 - loss: 0.1537 - val_accuracy: 0.9850 - val_loss: 0.0539\n",
      "Epoch 4/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 41ms/step - accuracy: 0.9597 - loss: 0.1319 - val_accuracy: 0.9843 - val_loss: 0.0503\n",
      "Epoch 5/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 39ms/step - accuracy: 0.9661 - loss: 0.1112 - val_accuracy: 0.9919 - val_loss: 0.0334\n",
      "Epoch 6/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 40ms/step - accuracy: 0.9706 - loss: 0.0971 - val_accuracy: 0.9910 - val_loss: 0.0326\n",
      "Epoch 7/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 40ms/step - accuracy: 0.9755 - loss: 0.0844 - val_accuracy: 0.9886 - val_loss: 0.0361\n",
      "Epoch 8/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 41ms/step - accuracy: 0.9751 - loss: 0.0812 - val_accuracy: 0.9924 - val_loss: 0.0305\n",
      "Epoch 9/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 40ms/step - accuracy: 0.9773 - loss: 0.0750 - val_accuracy: 0.9921 - val_loss: 0.0286\n",
      "Epoch 10/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 41ms/step - accuracy: 0.9790 - loss: 0.0683 - val_accuracy: 0.9921 - val_loss: 0.0274\n",
      "Epoch 11/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 40ms/step - accuracy: 0.9796 - loss: 0.0696 - val_accuracy: 0.9924 - val_loss: 0.0274\n",
      "Epoch 12/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 41ms/step - accuracy: 0.9809 - loss: 0.0620 - val_accuracy: 0.9907 - val_loss: 0.0360\n",
      "Epoch 13/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 40ms/step - accuracy: 0.9821 - loss: 0.0595 - val_accuracy: 0.9926 - val_loss: 0.0257\n",
      "Epoch 14/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 40ms/step - accuracy: 0.9833 - loss: 0.0545 - val_accuracy: 0.9933 - val_loss: 0.0232\n",
      "Epoch 15/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 39ms/step - accuracy: 0.9840 - loss: 0.0513 - val_accuracy: 0.9943 - val_loss: 0.0252\n",
      "Epoch 16/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 40ms/step - accuracy: 0.9830 - loss: 0.0531 - val_accuracy: 0.9940 - val_loss: 0.0248\n",
      "Epoch 17/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 40ms/step - accuracy: 0.9841 - loss: 0.0508 - val_accuracy: 0.9933 - val_loss: 0.0287\n",
      "Epoch 18/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 41ms/step - accuracy: 0.9848 - loss: 0.0503 - val_accuracy: 0.9940 - val_loss: 0.0234\n",
      "Epoch 19/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 40ms/step - accuracy: 0.9850 - loss: 0.0511 - val_accuracy: 0.9933 - val_loss: 0.0239\n",
      "Epoch 20/20\n",
      "\u001b[1m591/591\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 41ms/step - accuracy: 0.9853 - loss: 0.0478 - val_accuracy: 0.9924 - val_loss: 0.0271\n",
      "\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step\n",
      " Submission file saved\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "train = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\n",
    "\n",
    "# Split features & labels\n",
    "X = train.drop('label', axis=1).values.reshape(-1, 28, 28, 1) / 255.0  # Normalize\n",
    "y = train['label'].values\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(rotation_range=10, zoom_range=0.1, width_shift_range=0.1, height_shift_range=0.1)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=64), validation_data=(X_val, y_val), epochs=20)\n",
    "\n",
    "# Predict on test data\n",
    "X_test = test.values.reshape(-1, 28, 28, 1) / 255.0\n",
    "predictions = model.predict(X_test).argmax(axis=1)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({'ImageId': np.arange(1, len(predictions) + 1), 'Label': predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\" Submission file saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0343fc4c",
   "metadata": {
    "papermill": {
     "duration": 0.426388,
     "end_time": "2025-02-19T08:28:24.278392",
     "exception": false,
     "start_time": "2025-02-19T08:28:23.852004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce758c",
   "metadata": {
    "papermill": {
     "duration": 0.357517,
     "end_time": "2025-02-19T08:28:24.990504",
     "exception": false,
     "start_time": "2025-02-19T08:28:24.632987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e841ce1",
   "metadata": {
    "papermill": {
     "duration": 0.422666,
     "end_time": "2025-02-19T08:28:25.771375",
     "exception": false,
     "start_time": "2025-02-19T08:28:25.348709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2991ce01",
   "metadata": {
    "papermill": {
     "duration": 0.362051,
     "end_time": "2025-02-19T08:28:26.488290",
     "exception": false,
     "start_time": "2025-02-19T08:28:26.126239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 861823,
     "sourceId": 3004,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 607.907818,
   "end_time": "2025-02-19T08:28:28.669722",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-19T08:18:20.761904",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
